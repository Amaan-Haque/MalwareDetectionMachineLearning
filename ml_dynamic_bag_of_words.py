import os
import json
import pandas as pd
import numpy as np
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_val_score
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import precision_recall_fscore_support
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix
from sklearn.metrics import f1_score
from sklearn.neighbors import KNeighborsClassifier

'''
= = = = = = = = = = = = = = = NOTE = = = = = = = = = = = = = = =
> Run the section FEATURE EXTRACTION & CSV GENERATION (keeping the MACHINE
  LEARNING section commented)to generate `final_data_dynamic_bag_of_words.csv`.
  It takes a lot of time (10 min) to generate the .csv file.

> Once the .csv file is generated (or if it already exists in the directory), 
  run the MACHINE LEARNING section (keeping the FEATURE EXTRACTION & 
  CSV GENERATION commented) to evaluate ml model.
'''

# = = = = = = = = = = FEATURE EXTRACTION & CSV GENERATION - START = = = = = = = = = =

# # = = = = = = = = = = BENIGN = = = = = = = = = =
# file_list = open("file_list_benign1.txt", "r").read()

# os.chdir("./Dynamic_Analysis_Data_Part1/Benign")

# file_list = file_list.split('\n')

# hashes_list_benign = []
# dll_list_benign = []

# for file in file_list:
#         try:
#                 data = json.loads(open(file).read())
#                 dll_loaded = data["behavior"]["summary"]["dll_loaded"]
#                 dll_loaded = [i.lower() for i in dll_loaded]
#                 dll_loaded = " ".join(dll_loaded)
#                 dll_list_benign.append(dll_loaded)
#                 hashes_list_benign.append(file)
#         except:
#                 # print(file)
#                 pass

# columns = {"hash" : hashes_list_benign, "dll_loaded" : dll_list_benign, 'legit' : True}
# df_benign = pd.DataFrame(columns)

# print("benign done")

# # = = = = = = = = = = MALWARE = = = = = = = = = =
# os.chdir("../../")
# file_list = open("file_list_malware1.txt", "r").read()

# os.chdir("./Dynamic_Analysis_Data_Part1/Malware")

# file_list = file_list.split('\n')

# hashes_list_malware = []
# dll_list_malware = []

# for file in file_list:
#         try:
#                 data = json.loads(open(file).read())
#                 dll_loaded = data["behavior"]["summary"]["dll_loaded"]
#                 dll_loaded = [i.lower() for i in dll_loaded]
#                 dll_loaded = " ".join(dll_loaded)
#                 dll_list_malware.append(dll_loaded)
#                 hashes_list_malware.append(file)
#         except:
#                 # print(file)
#                 pass

# columns = {"hash" : hashes_list_malware, "dll_loaded" : dll_list_malware, 'legit' : False}
# df_malware = pd.DataFrame(columns)

# print("malware done")
# # # # = = = = = = = = = = COMBINED PROCESSING = = = = = = = = = =

# df = [df_benign, df_malware]
# df = pd.concat(df, ignore_index=True)
# os.chdir("../../")
# df.to_csv("final_data_dynamic_bag_of_words.csv")

# = = = = = = = = = = FEATURE EXTRACTION & CSV GENERATION - END = = = = = = = = = =

# = = = = = = = = = = MACHINE LEARNING = = = = = = = = = =

df = pd.read_csv("./final_data_dynamic_bag_of_words.csv")

# convert column names to lower case
df.columns = [colname.lower() for colname in df.columns]

X = df.drop(['hash', 'legit'], axis=1).values
y = df.drop(['hash', 'dll_loaded'], axis = 1).values

# drop 1st column
X = X[:, 1:]
y = y[:, 1:]

X = X.astype(str)
y = y.astype(int)

X_train, X_test, y_train, y_test = train_test_split(X, y)

X_train_string_list = [i[0] for i in X_train.tolist()]
X_test_string_list = [i[0] for i in X_test.tolist()]

vect = CountVectorizer(min_df=10)
vect.fit(X_train_string_list)
X_train_vect = vect.transform(X_train_string_list)
# print("X_train_vect:\n{}".format(repr(X_train_vect)))

# Features
# feature_names = vect.get_feature_names()
# print("Number of features: {}".format(len(feature_names)))
# print("First 20 features:\n{}".format(feature_names))

# Vocab
# print("Vocabulary size: {}".format(len(vect.vocabulary_)))
# print("Vocabulary content:\n {}".format(vect.vocabulary_))

# Bag of Words
# bag_of_words = vect.transform(X_train_string_list)
# print("bag_of_words: {}".format(repr(bag_of_words)))
# print("Dense representation of bag_of_words:\n{}".format(bag_of_words.toarray()))

# Evaluation of LogisticRegression using cross-validation
# scores = cross_val_score(LogisticRegression(), X_train_vect, y_train.ravel(), cv=5)
# print("Mean cross-validation accuracy : {:.2f}".format(np.mean(scores)))

param_grid = {'n_neighbors': [10, 25, 50, 100, 500]}
grid = GridSearchCV(KNeighborsClassifier(), param_grid, cv=5)
grid.fit(X_train_vect, y_train.ravel())
# print("Best cross-validation score : {:.2f}".format(grid.best_score_))
# print("Best parameters : ", grid.best_params_)
# print('---')

# Test Data
X_test_vect = vect.transform(X_test_string_list)

# Model Evaluation
y_pred = grid.predict(X_test_vect)
a = accuracy_score(y_test, y_pred)
p,r,f,s = precision_recall_fscore_support(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()
print("LogisticRegression")
print(f"accuracy : {a}\nprecision : {p}\nrecall : {r}\nf1 score : {f1}\nflase positive (fall out) : {fp / (fp + tn)}\nfalse negative (miss rate) : {fn / (fn + tp)}")




